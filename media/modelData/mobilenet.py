# -*- coding: utf-8 -*-
"""MobileNet_with_noice_explainer_mask_green_bg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1O3zwQ3zKhisYhbzO457ML45fng4ZMRD0
"""



"""# **MobileNet_with_noice_explainer**"""

import os
import itertools
import shutil
import random
import shap
import matplotlib.cm as cm
import cv2 as cv
import tensorflow as tf
from tensorflow import keras
from keras import backend as K
from keras.layers import Dense, Activation,Reshape, Dropout
from keras.optimizers import Adam
from keras.metrics import categorical_crossentropy
from keras.preprocessing.image import ImageDataGenerator
from keras.preprocessing import image
from keras.models import Model
from keras.applications import imagenet_utils
from keras.models import load_model
from sklearn import preprocessing

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

import numpy as np
import pandas as pd 
from glob import glob
import shutil
import matplotlib.pyplot as plt
# %matplotlib inline


def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
            horizontalalignment="center",
            color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')





"""Genrate random perturbations or input image"""

#Generate segmentation for image
import skimage.segmentation
import skimage.io
import copy
import sklearn.metrics
from sklearn.linear_model import LinearRegression
#Create function to apply perturbations to images

def perturb_image(model, classes, img,perturbation,segments): 
  active_pixels = np.where(perturbation == 1)[0]
  mask = np.zeros(segments.shape)
  for active in active_pixels:
      mask[segments == active] = 1 
  perturbed_image = copy.deepcopy(img)
  perturbed_image = perturbed_image*mask[:,:,np.newaxis]
  return perturbed_image


def lime(model, classes, img, kernal_sz=3, max_dis=100,ratio=0.01):
  double_img = np.array(img,dtype=np.double)
  superpixels = skimage.segmentation.quickshift(double_img, kernel_size= kernal_sz, max_dist=max_dis, ratio=ratio)
  num_superpixels = np.unique(superpixels).shape[0]
  #skimage.io.imshow(skimage.segmentation.mark_boundaries(np.array(double_img,dtype=np.double), superpixels))
  #Generate perturbations
  num_perturb = 300
  perturbations = np.random.binomial(1, 0.5, size=(num_perturb, num_superpixels))
  ##Show example of perturbations
  # plt.imshow(perturb_image(double_img,perturbations[0],superpixels))
  # plt.show()
  # plt.imshow(perturb_image(double_img,perturbations[1],superpixels))
  # plt.show()

  # plt.imshow(perturb_image(double_img,perturbations[99],superpixels))
  # plt.show()
  predictions = []
  for pert in perturbations:
    perturbed_img = perturb_image(double_img,pert,superpixels)
    pred = model.predict(perturbed_img[np.newaxis,:,:,:])
    predictions.append(pred)

  predictions = np.array(predictions)
  print(predictions.shape)
  #Compute distances to original image

  original_image = np.ones(num_superpixels)[np.newaxis,:] #Perturbation with all superpixels enabled 
  distances = sklearn.metrics.pairwise_distances(perturbations,original_image, metric='cosine').ravel()
  print(distances.shape)

  #Transform distances to a value between 0 an 1 (weights) using a kernel function
  kernel_width = 0.25
  weights = np.sqrt(np.exp(-(distances**2)/kernel_width**2)) #Kernel function
  # print(weights.shape)
  class_to_explain = classes[0] #yellow_curl_virus class
  simpler_model = LinearRegression()
  simpler_model.fit(X=perturbations, y=predictions[:,:,class_to_explain], sample_weight=weights)
  coeff = simpler_model.coef_[0]

  #Use coefficients from linear model to extract top features
  num_top_features = 10
  top_features = np.argsort(coeff)[-num_top_features:] 

  #Show only the superpixels corresponding to the top features
  mask = np.zeros(num_superpixels) 
  mask[top_features]= True #Activate top superpixels
  return perturb_image(double_img,mask,superpixels)
  # (perturb_image(double_img,mask,superpixels))




## **Saliency map**
def vanila_saliencymap( model, classes, img):
  input_img = img.reshape((1, *img.shape))
  image = tf.Variable(input_img, dtype=float)
  with tf.GradientTape() as tape:
      pred = model(image, training=False)
      class_idxs_sorted = np.argsort(pred.numpy().flatten())[::-1]
      loss = pred[0][class_idxs_sorted[0]]
  grads = tape.gradient(loss, image)
  # plt.imshow(grads)
  # plt.show()
  # grads.shape
  dgrad_abs = tf.math.abs(grads)
  dgrad_max_ = np.max(dgrad_abs, axis=3)[0]
  # print(dgrad_max_)
  # dgrad_max_.shape
  ## normalize to range between 0 and 1
  arr_min, arr_max  = np.min(dgrad_max_), np.max(dgrad_max_)
  grad_eval = (dgrad_max_ - arr_min) / (arr_max - arr_min + 1e-18)
  # grad_eval.shape
  # fig, axes = plt.subplots(1,2,figsize=(14,5))
  # axes[0].imshow(img)
  # i = axes[1].imshow(grad_eval,cmap="jet",alpha=0.8)
  # fig.colorbar(i)
  return grad_eval



"""## **GradCam and GradCam++**"""
def make_gradcam_heatmap( img_array, model,  last_conv_layer_name, pred_index=None):
    # First, we create a model that maps the input image to the activations
    # of the last conv layer as well as the output predictions
    grad_model = keras.models.Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]
    )

    # Then, we compute the gradient of the top predicted class for our input image
    # with respect to the activations of the last conv layer
    with tf.GradientTape() as tape:
        last_conv_layer_output, preds = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(preds[0])
        class_channel = preds[:, pred_index]

    # This is the gradient of the output neuron (top predicted or chosen)
    # with regard to the output feature map of the last conv layer
    grads = tape.gradient(class_channel, last_conv_layer_output)

    # This is a vector where each entry is the mean intensity of the gradient
    # over a specific feature map channel
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # We multiply each channel in the feature map array
    # by "how important this channel is" with regard to the top predicted class
    # then sum all the channels to obtain the heatmap class activation
    last_conv_layer_output = last_conv_layer_output[0]
    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    # For visualization purpose, we will also normalize the heatmap between 0 & 1
    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)
    return heatmap.numpy()

def make_gradcam_heatmap_pp(img_array, model,   
                  last_conv_layer_name,
                  pred_index=None):

    grad_model = keras.models.Model(
        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]
    )


    with tf.GradientTape() as gtape1:
        with tf.GradientTape() as gtape2:
            with tf.GradientTape() as gtape3:
                conv_output, predictions = grad_model(img_array)
                if pred_index==None:
                    pred_index = np.argmax(predictions[0])
                output = predictions[:, pred_index]
                conv_first_grad = gtape3.gradient(output, conv_output)
            conv_second_grad = gtape2.gradient(conv_first_grad, conv_output)
        conv_third_grad = gtape1.gradient(conv_second_grad, conv_output)

    global_sum = np.sum(conv_output, axis=(0, 1, 2))

    alpha_num = conv_second_grad[0]
    alpha_denom = conv_second_grad[0]*2.0 + conv_third_grad[0]*global_sum
    alpha_denom = np.where(alpha_denom != 0.0, alpha_denom, 1e-10)
    
    alphas = alpha_num/alpha_denom
    alpha_normalization_constant = np.sum(alphas, axis=(0,1))
    alphas /= alpha_normalization_constant

    weights = np.maximum(conv_first_grad[0], 0.0)

    deep_linearization_weights = np.sum(weights*alphas, axis=(0,1))
    grad_CAM_map = np.sum(deep_linearization_weights*conv_output[0], axis=2)

    heatmap = np.maximum(grad_CAM_map, 0)
    max_heat = np.max(heatmap)
    if max_heat == 0:
        max_heat = 1e-10
    heatmap /= max_heat

    return heatmap



import matplotlib.cm as cm

def display_gradcam( img, heatmap, alpha=0.02, beata=1):

    # Rescale heatmap to a range 0-255
    heatmap = np.uint8(255 * heatmap)


    # Use jet colormap to colorize heatmap
    jet = cm.get_cmap("jet")

    # Use RGB values of the colormap
    jet_colors = jet(np.arange(256))[:, :3]
    jet_heatmap = jet_colors[heatmap]

    # Create an image with RGB colorized heatmap
    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)
    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))
    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)

    # Superimpose the heatmap on original image
    superimposed_img = jet_heatmap * alpha + img  * beata
    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img) 
    # Display Grad CAM
    return superimposed_img

def gradcam( model, img):
  input_img = img.reshape((1,  *img.shape))
  heatmap_g = make_gradcam_heatmap(input_img, model, 'conv_pw_13_relu', pred_index=None)
  return display_gradcam(img,heatmap_g)

def gradcam_pp( model, img):
  input_img = img.reshape((1, *img.shape))
  heatmap_gg = make_gradcam_heatmap_pp(input_img, model, 'conv_pw_13_relu', pred_index=None)
  return display_gradcam(img,heatmap_gg)


import shap
# def f(model,x):
#     tmp = x.copy()
#     return model.predict(tmp)


# python function to get model output; replace this function with your own model function.
def shaply(images,classes,model):
  # define a maker that is used to mask out partitions of the input image.
  masker = shap.maskers.Image("blur(224,224)", [224,224,3])
  # create an explainer with model and image masker
  explainer = shap.Explainer(lambda x : model.predict(x), masker, output_names=classes)
  # here we explain two images using 500 evaluations of the underlying model to estimate the SHAP values
  shap_values = explainer(images, batch_size=50,max_evals=5000, outputs=shap.Explanation.argsort.flip[:5])
  return shap_values


